# ğŸ¡ Ridge Regression â€“ Predicting House Prices

## ğŸ“Œ Project Overview
This project focuses on predicting house prices using **regression techniques**.  
We build and compare **Ordinary Least Squares (OLS)** and **Ridge Regression** models to evaluate which performs better in the presence of **multicollinearity** among features.

- **Dataset:** [House Prices Dataset](https://drive.google.com/file/d/1FDbOghfF0PbG7F8T1TNjK2U9c0BzDZTJ/view?usp=sharing)  
- **Features:**  
  - `size_m2` â€“ size of the house in square meters  
  - `bedrooms` â€“ number of bedrooms  
  - `bathrooms` â€“ number of bathrooms  
  - `distance_city` â€“ distance from the city center (km)  
  - `age_years` â€“ age of the property in years  

- **Target Variable:**  
  - `price_k` â€“ house price (in thousands)

---

## ğŸ¯ Problem Statement
A **real-estate agency** wants to estimate house prices for potential buyers and sellers.  
However, the dataset contains **highly correlated features** (`size_m2` and `bedrooms`).  
This can lead to **unstable OLS coefficients** and poor generalization.  

To overcome this, we apply **Ridge Regression**, which adds an L2 penalty to stabilize coefficient estimates.

---

## âš™ï¸ Methodology

1. **Exploratory Data Analysis (EDA)**  
   - Checked correlations between features  
   - Identified multicollinearity problem (`size_m2` â†” `bedrooms`)

2. **Modeling Approaches**  
   - **OLS Regression (Baseline)**: Fits coefficients without penalty  
   - **Ridge Regression**: Penalizes large coefficients to reduce overfitting and handle collinearity

3. **Model Evaluation**  
   - Compared coefficients from OLS vs Ridge  
   - Evaluated on **RÂ² score** (goodness of fit)  
   - Analyzed performance on **test data**

---

## ğŸ“Š Results

- **OLS Regression**
  - Coefficients were unstable due to multicollinearity  
  - RÂ² (test): Lower, indicating poor generalization  

- **Ridge Regression**
  - Coefficients were **shrunken and more stable**  
  - RÂ² (test): Higher, better generalization to unseen data  

---

## âœ… Why Ridge Regression is Better Here
- **OLS Problem:** With highly correlated predictors, OLS assigns exaggerated and unstable coefficients, even if predictive power is low.  
- **Ridge Solution:** By introducing an **L2 penalty**, Ridge shrinks coefficients of correlated variables together, improving:
  - **Stability of coefficients**  
  - **Model interpretability**  
  - **Generalization performance**  

Thus, **Ridge Regression is preferred when predictors are correlated**, as in this real-estate dataset.

---

## ğŸ“Œ Tech Stack
- Python  
- NumPy, Pandas  
- Scikit-learn (OLS & Ridge regression, evaluation metrics)  

---
ğŸ“ˆ Future Improvements

Try Lasso Regression for feature selection

Experiment with RidgeCV (automatic alpha tuning)

Build a web app for interactive house price prediction

# House-price-prediction-end-to-end-project
